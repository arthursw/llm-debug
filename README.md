# llm-debug
## A Minimal Python Library to debug with LLMs


AI-powered Python debugging assistant.
Turn natural-language prompts into executable Python debug code that understands your current stack, variables, and source context.

## Features

üêç Generate Python debug commands from natural-language instructions.
üîç Context-aware: includes call stack, local/global variable previews, current function source, and nearby code.
‚ö° Works like an AI-augmented ipdb: just ask what you want to inspect.
ü§ñ Supports OpenAI

## Installation

`pip install llm-debug`

## Quick Start

```
from llm_debug.llm_debug import generate_commands

def buggy_function():
    my_data = {"a": 1, "b": [1, 2, 3]}
    x = 42
    # Ask the model for debugging help:
    code = generate_commands("print a summary of my_data and x")
    exec(code, globals(), locals())

buggy_function()
```

Example natural-language prompts you can give:
- "show me the shape of my numpy arrays"

- "list all functions in scope"

- "plot my_data['b'] as a histogram"

- "describe my_data"

## API

`generate_commands(prompt, frame=None, model="gpt-5-mini-2025-08-07", code_only=False, print_prompt=True, print_answer=True)`

Generate executable Python debug code from a natural-language prompt.

Arguments:

- `prompt (str)`: Instruction in natural language (e.g., "plot my variable").
- `frame (FrameType, optional)`: Execution frame. Defaults to caller‚Äôs frame.
- `model (str)`: LLM model to use. Defaults to "gpt-5-mini-2025-08-07".
- `code_only (bool)`: If True, forces the model to return only code (no explanation).
- `print_prompt (bool)`: Whether to print the full system + user prompt before sending to the model.
- `print_answer (bool)`: Whether to print the returned code.

Returns:

- A string containing Python code (to be executed with exec).

## Example Session

```
def compute():
    numbers = list(range(10))
    total = sum(numbers)
    # Natural-language debugging:
    code = generate_commands("plot numbers as a bar chart")
    exec(code, globals(), locals())

compute()
```

Possible output generated by the model:

```
import matplotlib.pyplot as plt
plt.bar(range(len(numbers)), numbers)
plt.show()
```

## Configuration

By default, llm-debug uses the OpenAI client.

### Using OpenRouter

To route via OpenRouter

`export OPENROUTER_API_KEY="your_api_key_here"`

## Why?

Debugging often requires repetitive inspection steps (print, dir, type, plot, etc.).
llm-debug automates this by letting an LLM generate context-aware commands on the fly, saving time and effort.

## License

MIT License.